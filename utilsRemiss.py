# utilsRemiss.py
# Remiss functionality put into a utils file.


# -*- coding: utf-8 -*-
"""2_Remiss_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NM9gSrvdaGa_bk4ZG1zkUgvmozuIXK7b

### Retrieve Twitter profiles given search terms
"""

# !pip3 install snscrape
# !pip install --upgrade snscrape
# !pip install tweepy
# Libraries installations
# !pip install googletrans==4.0.0-rc1
# !pip install empath
# !pip install py_lex
# !pip install tweetnlp
# !pip install liwc
# !pip install torch==1.12.1
# !pip install torchvision==0.13.1
# !pip install m3inference

# import display from Ipython.display

from googletrans import Translator
import random
from empath import Empath
from py_lex import Liwc, EmoLex
# A tokenizer of some kind, for example NLTK:
import nltk
import tweetnlp
import tweetnlp
import liwc
import re
from collections import Counter
import pprint
import torch
from m3inference import M3Twitter
import statistics as stat
import time
from datetime import datetime, date, timedelta
import numpy as np
import tweepy
import os
import json, time
import pickle
import snscrape.modules.twitter as sntwitter
import pandas as pd
import openai

import sys

from datetime import datetime

#
# consumer_key = "cn1c5JOe1vHLt6gJfePrgB2uE"
# consumer_secret = "ZdwX3lq3WgnyseygSN1OKkq6BKb4bj4o3CxY7kWZlfeIYZl4JC"
# access_token = "130174891-3Z9d5dYIG34RVeNGOR1UEIGfdmHK2I309n0aTaji"
# access_token_secret = "uyEartpJ3Zu9FWaWaGgPvh41mmV2lxWpBFrofMvYpMbv8"


consumer_key = "x44ZRBmHWG3vlV2XooLOcvi6X"
consumer_secret = "xQXsqq4oV2wVw4VL2BnM9nkLXsiHgiuJxWMHfEAxZCBLXZ7JD9"
access_token = "227861437-jrPJd9jAykWiIroElOaaynqvjKbMN19rAvtkEVeR"
access_token_secret = "r08LCeqRaDQmBMZ9u1oafFT8Mh46USLnST1iu52V9xp2M"

openai.api_key = "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"


# TODO replace this function for the right input collection approach. The output must be dataframe
# for now we get the search terms or phrases in a text file, this should be changed in order to let the user be the one who types the search terms
# for the demo users should be allowed to insert at least 1 search phrase to collect the tweets needed.

def remove_usernames(text):
    # Use regular expression to match words starting with "@"
    cleaned_text = ' '.join(word for word in text.split() if not word.startswith('@'))
    return cleaned_text


def read_search_keys(phrases_path):
    # input: path to the file with the search terms
    # output: pandas dataframe with the phrases collected
    import pandas as pd
    df = pd.read_csv(phrases_path, encoding='utf-8', header=None)
    df.columns = ['frases']
    return (df)


# data cleaning
def clean_text(text):
    import numpy as np
    import re
    if type(text) == np.float:
        return ""
    temp = text.lower()
    temp = re.sub(r'http\S+', '', temp)
    temp = temp.replace("'", "")  # to avoid removing contractions in english
    temp = re.sub("@[A-Za-z0-9_]+", "", temp)
    temp = re.sub(r'[^\w]', ' ', temp)
    temp = re.sub('[()!?]', ' ', temp)
    temp = re.sub('\[.*?\]', ' ', temp)
    temp = temp.replace("\n", " ")
    temp = temp.replace(".", " ")
    temp = temp.replace("\"", " ")
    temp = re.sub('[0-9]', '', temp)
    temp = temp.strip()
    temp = re.sub(' +', ' ', temp)
    return temp


# main function to search tweets given the search terms
# Using snscrape library
# input: dataframe with searh phrases the condition studied (anorexia, suicide or fake news) language and the date frame for the tweets published (start and end)
# output: dataframe with collected tweets

def search_tweets(phrases_df, language, start_date, end_date, max_tweets, pathDotEnv):
    # imports
    import snscrape.modules.twitter as sntwitter
    import pandas as pd
    from dotenv import load_dotenv

    load_dotenv(pathDotEnv)
    # to collect tweets https://www.freecodecamp.org/news/python-web-scraping-tutorial/
    attributes_container = []

    # change to collect data from all the search keys
    all_tweets_df = pd.DataFrame(pd.DataFrame(
        columns=["Id", "User", "User description", "Date Created", "Replies received", "Retweets", "Likes", "Tweets"]))
    existing_tweets = []
    # collect tweets per each search term or phrase
    for phrase in phrases_df['frases'].values:
        # print ("FRASE is", phrase)
        # print ("Dates are "+start_date + " "+end_date )
        # Using TwitterSearchScraper to find tweets and add them to the list
        scrapperAnswer = sntwitter.TwitterSearchScraper(
            phrase + ' near:Madrid within:1000mi lang:' + language + ' since:' + start_date + ' until:' + end_date).get_items();
        for i, tweet in enumerate(scrapperAnswer):
            clean_tw = clean_text(tweet.rawContent);
            # collect up to max_amount tweets per phrase
            if i > max_tweets:
                break

            if clean_tw not in existing_tweets:
                attributes_container.append(
                    [tweet.id, tweet.username, clean_text(tweet.user.renderedDescription), tweet.date, tweet.replyCount,
                     tweet.retweetCount, tweet.likeCount, clean_tw]);
                existing_tweets.append(clean_tw);
        if all_tweets_df.empty:
            # TODO: Twees should be able to be ordered by tweets with the highest number of likes or retweets received (popularity), and by relevance meaning we consider users with most posts related to the search terms
            # TODO: Define this sorting criteria
            all_tweets_df = pd.DataFrame(attributes_container,
                                         columns=["Id", "User", "User description", "Date Created", "Replies received",
                                                  "Retweets", "Likes", "Tweets"]);

        else:
            temporalf_df1 = pd.DataFrame(attributes_container,
                                         columns=["Id", "User", "User description", "Date Created", "Replies received",
                                                  "Retweets", "Likes", "Tweets"]);
            all_tweets_df = all_tweets_df.append(temporalf_df1);
        attributes_container = []
    all_tweets_df = all_tweets_df.set_index(['Id'])
    return all_tweets_df


def initListOfTweets():
    all_tweets_df = pd.DataFrame(pd.DataFrame(
        columns=["Id", "User", "User description", "Date Created", "Replies received", "Retweets", "Likes", "Tweets"]));
    return all_tweets_df;


def search_tweetsWithInventedData(phrases_df, language, start_date, end_date, max_tweets):
    # imports
    # import snscrape.modules.twitter as sntwitter
    import pandas as pd

    # to collect tweets https://www.freecodecamp.org/news/python-web-scraping-tutorial/
    attributes_container = []

    # change to collect data from all the search keys
    all_tweets_df = pd.DataFrame(
        pd.DataFrame(columns=["Id", "User", "Date Created", "Replies received", "Retweets", "Likes", "Tweets"]));
    existing_tweets = [];
    #    tweets = [
    #    		["243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", date(2011, 1, 1), 9, 26, 68, "Mensaje recibido hoy que ilustra la situaciÃ³n que estÃ¡n viviendo los vecinos en todos los barrios de #Badalona en relaciÃ³na la inseguridad. Entiendo la indignaciÃ³n y preocupaciÃ³n de los vecinos. A partir de mayo tienen mi compromiso que daremos la vuelta a la situaciÃ³n actual."],
    #        ["243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", date(2011, 1, 1), 1, 0, 11, "ðŸ€ La @Penya1930 cau eliminada a les semifinal de la #EuroCup perÃ² amb el cap ben alt. Ens ha faltat uns segons mÃ©s de partit! Sou un orgull! #Badalona"],
    #        ["243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", date(2011, 1, 1), 15, 41, 146, "SituaciÃ³n de peligro para los vecinos de un bloque de viviendas del barrio de #Bonavista. #Badalona"],
    #        ["63482996", "@annagrauarias", "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.", date(2009, 8, 1), 7, 15, 33, "âœŒï¸ OKUPAS FUERA, VECINOS DENTRO #PalabradeGrau"],
    #        ["63482996", "@annagrauarias", "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.", date(2009, 8, 1), 5, 20, 12, "âœŒï¸ Para que la ley sea igual para todos, y todos (no sÃ³lo @lauraborras, @MeritxellSerret, etc....) tengan quien les defienda gratis, hace falta un turno de oficio dignificado. Apoyamos al @ICABarcelona y a tantos generosos abogados que dan mÃ¡s de lo que reciben #PalabradeGrau"],
    #        ["63482996", "@annagrauarias", "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.", date(2009, 8, 1), 1, 2, 5, "âœŒï¸Gracias a los asistentes tanto presenciales como telemÃ¡ticos, gracias a @NewEconomyForum por la invitaciÃ³n. Urge iniciar una conversaciÃ³n sincera, profunda y completa sobre el futuro de esta ciudad #PalabradeGrau"],
    #        ["102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 49, 5806, 12400, "He decidido dejar Twitter con carÃ¡cter indefinido. AquÃ­ cuento mis razones. He decidit deixar Twitter amb carÃ cter indefinit. AquÃ­ explico les meves raons."],
    #        ["102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 0, 599, 1206, "Per celebrar lâ€™aniversari, em regalo uns dies sense twitter. Feia temps que ho volia provar, ja us explicarÃ© quÃ¨ tal lâ€™experiÃ¨ncia ðŸ˜‰"],
    #				["102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 333, 25, 98, "Totes les morts sÃ³n doloroses perÃ² les dels nens i joves especialment. Un noi de 14 anys ha mort en un sinistre amb un cotxe a SarriÃ  Sant-Gervasi. El meu condol a la famÃ­lia i amistats. Cal un compromÃ­s colÂ·lectiu per aconseguir una BCN sense vÃ­ctimes de trÃ nsit. #BCNvÃ­ctimes0"]
    #    ];
    tweets = [
        ["243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme",
         date(2011, 1, 1), 80, 5, 5,
         "Mensaje recibido hoy que ilustra la situaciÃ³n que estÃ¡n viviendo los vecinos en todos los barrios de #Badalona en relaciÃ³na la inseguridad. Entiendo la indignaciÃ³n y preocupaciÃ³n de los vecinos. A partir de mayo tienen mi compromiso que daremos la vuelta a la situaciÃ³n actual."],
        ["243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme",
         date(2011, 1, 1), 80, 5, 5,
         "ðŸ€ La @Penya1930 cau eliminada a les semifinal de la #EuroCup perÃ² amb el cap ben alt. Ens ha faltat uns segons mÃ©s de partit! Sou un orgull! #Badalona"],
        # ["243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", date(2011, 1, 1), 81, 5, 5, "SituaciÃ³n de peligro para los vecinos de un bloque de viviendas del barrio de #Bonavista. #Badalona"],
        ["63482996", "@annagrauarias",
         "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.",
         date(2009, 8, 1), 7, 15, 33, "âœŒï¸ OKUPAS FUERA, VECINOS DENTRO #PalabradeGrau"],
        ["63482996", "@annagrauarias",
         "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.",
         date(2009, 8, 1), 5, 20, 12,
         "âœŒï¸ Para que la ley sea igual para todos, y todos (no sÃ³lo @lauraborras, @MeritxellSerret, etc....) tengan quien les defienda gratis, hace falta un turno de oficio dignificado. Apoyamos al @ICABarcelona y a tantos generosos abogados que dan mÃ¡s de lo que reciben #PalabradeGrau"],
        ["63482996", "@annagrauarias",
         "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.",
         date(2009, 8, 1), 1, 2, 5,
         "âœŒï¸Gracias a los asistentes tanto presenciales como telemÃ¡ticos, gracias a @NewEconomyForum por la invitaciÃ³n. Urge iniciar una conversaciÃ³n sincera, profunda y completa sobre el futuro de esta ciudad #PalabradeGrau"],
        ["102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau",
         date(2010, 1, 1), 1, 1, 100,
         "He decidido dejar Twitter con carÃ¡cter indefinido. AquÃ­ cuento mis razones. He decidit deixar Twitter amb carÃ cter indefinit. AquÃ­ explico les meves raons."],
        # ["102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 10, 10, 10, "Per celebrar lâ€™aniversari, em regalo uns dies sense twitter. Feia temps que ho volia provar, ja us explicarÃ© quÃ¨ tal lâ€™experiÃ¨ncia ðŸ˜‰"],
        # ["102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 10, 10, 10, "Totes les morts sÃ³n doloroses perÃ² les dels nens i joves especialment. Un noi de 14 anys ha mort en un sinistre amb un cotxe a SarriÃ  Sant-Gervasi. El meu condol a la famÃ­lia i amistats. Cal un compromÃ­s colÂ·lectiu per aconseguir una BCN sense vÃ­ctimes de trÃ nsit. #BCNvÃ­ctimes0"]
    ];

    #    tweets = [
    #    		{"Id":"243153735", "User": "@Albiol_XG", "User description": "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", "Date Created": date(2011, 1, 1), "Replies received": 9, "Retweets": 26, "Likes": 68, "content": "Mensaje recibido hoy que ilustra la situaciÃ³n que estÃ¡n viviendo los vecinos en todos los barrios de #Badalona en relaciÃ³na la inseguridad. Entiendo la indignaciÃ³n y preocupaciÃ³n de los vecinos. A partir de mayo tienen mi compromiso que daremos la vuelta a la situaciÃ³n actual."},
    #        {"Id":"243153735", "User":"@Albiol_XG", "User description":"Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", "Date Created": date(2011, 1, 1), "Replies received":1,  "Retweets":0, "Likes":11, "content":"ðŸ€ La @Penya1930 cau eliminada a les semifinal de la #EuroCup perÃ² amb el cap ben alt. Ens ha faltat uns segons mÃ©s de partit! Sou un orgull! #Badalona"}#,
    #        #{"243153735", "@Albiol_XG", "Graduado en Derecho. Candidato a la alcaldÃ­a de Badalona 2023. #Badalonisme", date(2011, 1, 1), 15, 41, 146, "SituaciÃ³n de peligro para los vecinos de un bloque de viviendas del barrio de #Bonavista. #Badalona"},
    #        #{"63482996", "@annagrauarias", "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.", date(2009, 8, 1), 7, 15, 33, "âœŒï¸ OKUPAS FUERA, VECINOS DENTRO #PalabradeGrau"},
    #        #{"63482996", "@annagrauarias", "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.", date(2009, 8, 1), 5, 20, 12, "âœŒï¸ Para que la ley sea igual para todos, y todos (no sÃ³lo @lauraborras, @MeritxellSerret, etc....) tengan quien les defienda gratis, hace falta un turno de oficio dignificado. Apoyamos al @ICABarcelona y a tantos generosos abogados que dan mÃ¡s de lo que reciben #PalabradeGrau"},
    #        #{"63482996", "@annagrauarias", "Escritora, Periodista, Catalan Unchained, Proud Spaniard, Gironina del MÃ³n, Madre de MadrileÃ±@, diputada de Ciutadans al Parlament de Catalunya.", date(2009, 8, 1), 1, 2, 5, "âœŒï¸Gracias a los asistentes tanto presenciales como telemÃ¡ticos, gracias a @NewEconomyForum por la invitaciÃ³n. Urge iniciar una conversaciÃ³n sincera, profunda y completa sobre el futuro de esta ciudad #PalabradeGrau"},
    #        #{"102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 49, 5806, 12400, "He decidido dejar Twitter con carÃ¡cter indefinido. AquÃ­ cuento mis razones. He decidit deixar Twitter amb carÃ cter indefinit. AquÃ­ explico les meves raons."},
    #        #{"102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 0, 599, 1206, "Per celebrar lâ€™aniversari, em regalo uns dies sense twitter. Feia temps que ho volia provar, ja us explicarÃ© quÃ¨ tal lâ€™experiÃ¨ncia ðŸ˜‰"},
    #				#{"102095809", "@AdaColau", "Alcaldessa de Barcelona. Mayor of Barcelona. http://instagram.com/adacolau", date(2010, 1, 1), 333, 25, 98, "Totes les morts sÃ³n doloroses perÃ² les dels nens i joves especialment. Un noi de 14 anys ha mort en un sinistre amb un cotxe a SarriÃ  Sant-Gervasi. El meu condol a la famÃ­lia i amistats. Cal un compromÃ­s colÂ·lectiu per aconseguir una BCN sense vÃ­ctimes de trÃ nsit. #BCNvÃ­ctimes0"}
    #    ]

    # Using TwitterSearchScraper to find tweets and add them to the list
    #    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(phrase+' lang:'+language+' since:'+start_date+' until:'+end_date).get_items()):
    for i, tweet in enumerate(tweets):
        # print(tweet)
        clean_tw = clean_text(tweet[7])
        # collect up to max_amount tweets per phrase
        if i > max_tweets:
            break

        if clean_tw not in existing_tweets:
            attributes_container.append(
                [tweet[0], tweet[1], tweet[2], tweet[3], tweet[4], tweet[5], tweet[6], clean_tw])
            existing_tweets.append(clean_tw)

        if all_tweets_df.empty:
            # TODO: Twees should be able to be ordered by tweets with the highest number of likes or retweets received (popularity), and by relevance meaning we consider users with most posts related to the search terms
            # TODO: Define this sorting criteria
            all_tweets_df = pd.DataFrame(attributes_container,
                                         columns=["Id", "User", "User description", "Date Created", "Replies received",
                                                  "Retweets", "Likes", "Tweets"])

        else:
            temporalf_df1 = pd.DataFrame(attributes_container,
                                         columns=["Id", "User", "User description", "Date Created", "Replies received",
                                                  "Retweets", "Likes", "Tweets"])
            all_tweets_df = all_tweets_df.append(temporalf_df1)

        attributes_container = []
    all_tweets_df = all_tweets_df.set_index(['Id'])

    return all_tweets_df


##function to get the top x candidates considering users with the highest amount of tweets using the search terms
# def top_users_ordered_by_tweets_number(all_tweets_df, number_candidates):
#    all_tweets_df=all_tweets_df.sort_values(by=['User'])
#    candidates=all_tweets_df['User'].value_counts().rename_axis('User').reset_index(name='tweets_count').set_index(['User'])
#    #print(candidates)
#    user_ids=candidates.index.tolist()[:number_candidates]
#    nTweets = candidates['tweets_count'].tolist()[:number_candidates]    
#    return [user_ids, nTweets]

def top_users_ordered_by_tweets_number(all_tweets_df, number_candidates, lang):
    all_tweets_df['Interactions'] = all_tweets_df['Retweets'] + all_tweets_df['Likes'] + all_tweets_df[
        'Replies received'];
    all_tweets_df['NTweets'] = 0;
    all_tweets_df['Interactions per User'] = 0;

    all_tweets_df_sorted = all_tweets_df.sort_values('Interactions', ascending=False);
    # Here we have the tweets ordered by iterations. Le	t's get the users with the tweets with more iterations.
    all_tweets_df_grouped = all_tweets_df_sorted.groupby('User');
    nTuitsPerUser_series = all_tweets_df_grouped['User'].count();

    # Calculate the accumulated iterations for the user.
    all_tweets_df_grouped_per_user = all_tweets_df.groupby('User').sum();

    # Let's find the candidates...
    all_tweets_df = all_tweets_df.sort_values(by=['User'])
    candidates = all_tweets_df['User'].value_counts().rename_axis('User').reset_index(name='tweets_count').set_index(
        ['User'])
    # print(candidates)
    top_users = candidates.index.tolist()[:number_candidates]

    # top_users = list(nTuitsPerUser_series)[:number_candidates];
    # print(top_users)
    # We want to return the users with the top iterations of each tweet, but also with the tweet and the number of replies, likes and retweets.
    top_tweets_df = pd.DataFrame(
        columns=["Id", "User", "User description", "Date Created", "Replies received", "Retweets", "Likes", "Tweets",
                 "NTweets"]);
    for user in top_users:
        userDataFrame = all_tweets_df_sorted[all_tweets_df_sorted['User'] == user];
        # print(userDataFrame)
        # print(all_tweets_df_grouped_per_user)
        # userDataFrame.loc[user, 'NTweets'] = nTuitsPerUser_series[user];
        userDataFrame['NTweets'] = nTuitsPerUser_series[user];
        userDataFrame['Interactions per User'] = all_tweets_df_grouped_per_user.loc[user, 'Interactions']
        # userDataFrame['anonymized_text'] = "ANONIMIZED TEXT"
        # userDataFrame['user_summary'] = "ANONIMIZED USER SUMMARY"
        # print(nTuitsPerUser_df[user])
        rowToShow = userDataFrame[userDataFrame['Interactions'] == userDataFrame['Interactions'].max()];
        # print(rowToShow);
        top_tweets_df = top_tweets_df.append(rowToShow.head(1));

    # print ("ADDing anonimous tweets")
    top_tweets_df = paraphrase_tweets(top_tweets_df, lang)
    return top_users, top_tweets_df


# function to get the top x candidates considering the users that have the tweet with more interactions.
def top_users_ordered_by_tweets_with_more_interactions(all_tweets_df, number_candidates, lang):
    all_tweets_df['Interactions'] = all_tweets_df['Retweets'] + all_tweets_df['Likes'] + all_tweets_df[
        'Replies received'];
    all_tweets_df['NTweets'] = 0;
    all_tweets_df['Interactions per User'] = 0;
    all_tweets_df_sorted = all_tweets_df.sort_values('Interactions', ascending=False);
    # print("*-*-*-**-*-*-*-**-*-*all_tweets_df_sorted")
    # print("*-*-*-**-*-*-*-**-*-*all_tweets_df_sorted", all_tweets_df_sorted[['User','Replies received', 'Retweets', 'Likes', 'Interactions']].head(500))
    listOfUsersSorted = list(all_tweets_df_sorted['User'])
    # print ("ara",listOfUsersSorted[0:10])
    listOfUsersSortedNotRepeated = list(dict.fromkeys(listOfUsersSorted))
    # print("cosa",listOfUsersSortedNotRepeated[0:10])

    # Here we have the tweets ordered by iterations. Le	t's get the users with the tweets with more iterations.
    all_tweets_df_grouped = all_tweets_df_sorted.groupby('User');
    nTuitsPerUser_series = all_tweets_df_grouped['User'].count();
    # print(nTuitsPerUser_series)

    # Calculate the accumulated iterations for the user.
    all_tweets_df_grouped_per_user = all_tweets_df.groupby('User').sum();

    # print("****---***--***---***---**--all_tweets_df_grouped")
    # print("****---***--***---***---**--all_tweets_df_grouped", )
    top_users = listOfUsersSortedNotRepeated[:number_candidates];
    # We want to return the users with the top iterations of each tweet, but also with the tweet and the number of replies, likes and retweets.
    top_tweets_df = pd.DataFrame(
        columns=["Id", "User", "User description", "Date Created", "Replies received", "Retweets", "Likes", "Tweets",
                 "NTweets"]);
    for user in top_users:
        userDataFrame = all_tweets_df_sorted[all_tweets_df_sorted['User'] == user];
        # userDataFrame.loc[user, 'NTweets'] = nTuitsPerUser_series[user];
        userDataFrame['NTweets'] = nTuitsPerUser_series[user];
        userDataFrame['Interactions per User'] = all_tweets_df_grouped_per_user.loc[user, 'Interactions']
        # userDataFrame['anonymized_text'] = "ANONIMIZED TEXT"
        # userDataFrame['user_summary'] = "ANONIMIZED USER SUMMARY"
        # print(nTuitsPerUser_df[user])
        rowToShow = userDataFrame[userDataFrame['Interactions'] == userDataFrame['Interactions'].max()];
        top_tweets_df = top_tweets_df.append(
            rowToShow.head(1));  # we add the head(1) for if more than one row have the maximmum value.

    top_tweets_df = paraphrase_tweets(top_tweets_df, lang)
    return top_users, top_tweets_df


# function to get the top x candidates considering users with more interactions (adding the interactions of all their tweets).
def top_users_ordered_by_users_with_more_interactions(all_tweets_df, number_candidates, lang):
    all_tweets_df['Interactions'] = all_tweets_df['Retweets'] + all_tweets_df['Likes'] + all_tweets_df[
        'Replies received'];
    all_tweets_df['Interactions per User'] = 0;
    all_tweets_df['NTweets'] = 0
    all_tweets_df_sorted = all_tweets_df.sort_values('Interactions', ascending=False);
    # Here we have the tweets ordered by iterations. Le	t's get the users with the tweets with more iterations.
    all_tweets_df_grouped_to_count = all_tweets_df_sorted.groupby('User');
    nTuitsPerUser_series = all_tweets_df_grouped_to_count['User'].count();
    # print(nTuitsPerUser_series)

    # Let's get the maximum interactions with a user.
    all_tweets_df_grouped_to_get_iterations_per_user = all_tweets_df.groupby('User').sum();
    # print(type(all_tweets_df_grouped_to_get_iterations_per_user))

    all_tweets_df_grouped = all_tweets_df_sorted.groupby('User').sum();
    all_tweets_df_grouped_sorted = all_tweets_df_grouped.sort_values('Interactions', ascending=False);
    # Here there will not be repeated users.

    top_users = list(all_tweets_df_grouped_sorted.index)[:number_candidates];
    # We want to return the users with the top iterations of each tweet, but also with the tweet and the number of replies, likes and retweets.
    top_tweets_df = pd.DataFrame(
        columns=["Id", "User", "User description", "Date Created", "Replies received", "Retweets", "Likes", "Tweets",
                 "NTweets"]);
    # We choose the tweet with more interactions for each user.
    for user in top_users:
        userDataFrame = all_tweets_df_sorted[all_tweets_df_sorted['User'] == user];
        # userDataFrame.loc[user, 'NTweets'] = nTuitsPerUser_series[user];
        userDataFrame['NTweets'] = nTuitsPerUser_series[user];
        # userDataFrame['anonymized_text'] = "ANONIMIZED TEXT"
        # userDataFrame['user_summary'] = "ANONIMIZED USER SUMMARY"
        rowToShow = userDataFrame[userDataFrame['Interactions'] == userDataFrame['Interactions'].max()];
        # print(rowToShow);
        rowToAdd = rowToShow.head(1);
        # rowToAdd['Interactions per User'] =rowToAdd['Interactions'];
        rowToAdd['Interactions per User'] = all_tweets_df_grouped_to_get_iterations_per_user.loc[user, 'Interactions']
        top_tweets_df = top_tweets_df.append(rowToAdd);

    top_tweets_df = paraphrase_tweets(top_tweets_df, lang)
    return top_users, top_tweets_df


# def top_users_ordered_by_tweets_number(all_tweets_df, number_candidates):
#	#This function will sort the users by their number of tweets. It also needs to return:
#	#User name
#	#Number of Tweets found.
#	#Number of Accumulated Reactions.
#	#Number of reactions to the most popular tweet.
#	#Sample Tweet.
#	#Date for sample tweet.
#	#Number of comments.
#	#Number of retweets.
#	#Number of Likes.
#	
#    all_tweets_df=all_tweets_df.sort_values(by=['User'])
#    candidates=all_tweets_df['User'].value_counts().rename_axis('User').reset_index(name='tweets_count').set_index(['User'])
#    print(candidates)
#    user_ids=candidates.index.tolist()[:number_candidates]
#    return user_ids


# function to load a single user data
def collect_single_user_data(user_id, max_tweets, api):
    try:
        for status in tweepy.Cursor(api.user_timeline, screen_name='@' + user_id, tweet_mode="extended").items(
                max_tweets):
            json_tweet = json.dumps(status._json)
            with open(".//new_users_json//" + user_id + ".json", 'a') as outfile:
                json.dump(json_tweet, outfile)
    except BaseException as e:
        print(' ERROR failed on_status,', str(e))  # print the error code obtained from twitter


"""### Load collected user data """


# function to load a single user data
def collect_single_user_data_on_the_fly(user_id, max_tweets, api):
    data = []
    try:
        for status in tweepy.Cursor(api.user_timeline, screen_name='@' + user_id, tweet_mode="extended").items(
                max_tweets):
            data.append(status._json)

    except BaseException as e:
        print(' ERROR failed on_status,', str(e))  # print the error code obtained from twitter
    # print(data)
    return data


# load all the user data stored in a json file
def load_user_json_file(user_id):
    # adapt_data_to_format
    with open("new_users_json//" + user_id + '.json') as f:
        content = f.readlines()
        for line in content:
            users_data = line.split('""')
            users_data[0] = users_data[0].replace('"{', '{')
            users_data[-1] = users_data[-1].replace('}"', '}')
            counter = 0
            for user in users_data:
                users_data[counter] = '"' + users_data[counter] + '"'
                counter += 1
    # load data in right format
    data = [json.loads(json.loads(user)) for user in users_data]
    return data


# get only the required data per user. This function should be modified if you want to get any further data from the collected tweets
# see for reference: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet 
def load_user_profile_data(user_info, user):
    # user data is repeated through each user
    user_data = {}
    tweets_data = []
    for tweet in user_info:
        # users_data - repeated in all tweets so we only keep data from the las tweet
        user_data["twitter_id"] = tweet["user"]["id_str"]
        user_data["name"] = tweet["user"]["name"]
        user_data["description"] = tweet["user"]["description"]
        user_data["location"] = tweet["user"]["location"]
        if tweet["user"]["location"] == "":
            user_data["location"] = "not defined"
        user_data["is_verified"] = tweet["user"]["verified"]
        user_data["followers_count"] = tweet["user"]["followers_count"]
        user_data["friends_count"] = tweet["user"]["friends_count"]
        user_data["listed_count"] = tweet["user"]["listed_count"]
        user_data["favorites_count"] = tweet["user"]["favourites_count"]
        user_data["statuses_count"] = tweet["user"]["statuses_count"]
        user_data["account_creation_date"] = tweet["user"]["created_at"]
        user_data["profile_image_url"] = tweet["user"]["profile_image_url_https"]
        user_data["url_in_profile"] = 0
        if "url" in tweet["user"]["entities"]:
            user_data["url_in_profile"] = len(tweet["user"]["entities"]["url"]["urls"])

        # tweets data (we collect data from each tweet)
        tweet_info = {}
        tweet_info["id"] = tweet["id_str"]
        tweet_info["text"] = tweet["full_text"]
        tweet_info["created_at"] = tweet["created_at"]
        tweet_info["posting_source"] = tweet["source"]
        tweet_info["is_quoted"] = str(tweet["is_quote_status"]).lower()
        tweet_info["is_retweet"] = "false"
        if "retweeted_status" in tweet:
            tweet_info["is_retweet"] = "true"
        tweet_info["is_reply"] = "false"
        if tweet["in_reply_to_status_id"] != None:
            tweet_info["is_reply"] = "true"
        tweet_info["retweet_count"] = tweet["retweet_count"]
        tweet_info["favorite_count"] = tweet["favorite_count"]
        tweet_info["hashtags"] = []
        for hashtag in tweet["entities"]["hashtags"]:
            tweet_info["hashtags"].append(hashtag)
        tweet_info["hashtags_count"] = len(tweet["entities"]["hashtags"])
        tweet_info["users_mentions_count"] = len(tweet["entities"]["user_mentions"])
        tweet_info["urls_count"] = len(tweet["entities"]["urls"])
        tweet_info["media_elements_count"] = 0
        if "media" in tweet["entities"]:
            tweet_info["media_elements_count"] = len(tweet["entities"]["media"])
        tweet_info["is_sensitive"] = "not defined"
        if "possibly_sensitive" in tweet:
            tweet_info["is_sensitive"] = tweet["possibly_sensitive"]
        tweets_data.append(tweet_info)

    user_data["tweets"] = tweets_data
    user_data["user_id"] = user
    return user_data


## #function to load a single user data 
## OLD VERSION
##def collect_single_user_data_on_the_fly(user_id, max_tweets, api):  
##    data=[]
##    try:
##        for status in tweepy.Cursor(api.user_timeline, screen_name='@'+user_id, tweet_mode="extended").items(max_tweets):                        
##            data.append(status._json)
##             
##    except BaseException as e:
##            print(' ERROR failed on_status,', str(e))  # print the error code obtained from twitter      
##    #print(data)
##    return data    
# New Version
def load_user_data_on_the_fly(user_id, user_data):
    # print("user_data", user_data)
    user_rel_data = load_user_profile_data(user_data, user_id)
    return (user_rel_data)


##OLD VERSION
###load json dataget_medians_dfs
##def load_user_data(user_id):                   
##    user_data=load_user_json_file(user_id)
##    user_rel_data=load_user_profile_data(user_data,user_id)   
##    return(user_rel_data)

# New Version.
# function to load a single user data
def collect_single_user_data(user_id, max_tweets, api):
    data = []
    try:
        for status in tweepy.Cursor(api.user_timeline, screen_name='@' + user_id, tweet_mode="extended").items(
                max_tweets):
            data.append(status._json)
    except BaseException as e:
        print(' ERROR failed on_status,', str(e))  # print the error code obtained from twitter
    # print(data)
    return data


# load json data
def load_user_data(user_id, user_data):
    user_rel_data = load_user_profile_data(user_data, user_id)
    return (user_rel_data)


# DATA CLEANING AND PREPROCESSING
def clean_text(text):
    import numpy as np
    import re
    if type(text) == np.float:
        return ""
    temp = text.lower()
    temp = re.sub(r'http\S+', '', temp)
    temp = temp.replace("'", "")  # to avoid removing contractions in english
    temp = re.sub("@[A-Za-z0-9_]+", "", temp)
    temp = re.sub(r'[^\w]', ' ', temp)
    temp = re.sub('[()!?]', ' ', temp)
    temp = re.sub('\[.*?\]', ' ', temp)
    temp = temp.replace("\n", " ")
    temp = temp.replace(".", " ")
    temp = temp.replace("\"", " ")
    temp = re.sub('[0-9]', '', temp)
    temp = temp.strip()
    temp = re.sub(' +', ' ', temp)
    return temp


# get users texts
def get_texts(user_tweets):
    # print ("user_tweets",user_tweets)
    tweets = []
    for tweet in user_tweets:
        # print ("tweet", tweet)
        tweets.append(clean_text(tweet['text']))
    return tweets


# TRANSLATION
# translation functions

# verify character limits for translation tool
def verify_characters_limits(text):
    # print("LEN is ", len(text))
    if len(text) < 1800:
        return True
    else:
        return False


# create chunks for translation to avoid translation limits
def build_query_chunks(tweets_clean):
    appended_tweets_chunks = []
    appended_tweets = ""
    char_lenght = 0
    for tweet in tweets_clean:
        if len(tweet) > 10:
            # print ("T---",tweet)
            if verify_characters_limits(appended_tweets) == True:
                # print("T")
                appended_tweets += tweet + ";;"
                # print("AT----",appended_tweets)
                # print ("TC__", tweets_clean[-1])
                if tweet == tweets_clean[-1]:
                    # print("YYYY")
                    if verify_characters_limits(appended_tweets) == True:
                        # print("verify_characters_limits__TT")
                        appended_tweets_chunks.append(appended_tweets)
                        # print("ATC----",appended_tweets_chunks)
                    else:
                        # we add it anyway because otherwise is empty.
                        print("Added but too long")
                        # print("verify_characters_limits__TT")
                        appended_tweets_chunks.append(appended_tweets)
                        # print("ATC----",appended_tweets_chunks)
            else:
                # print("F")
                appended_tweets_chunks.append(appended_tweets)
                # print("atc--",appended_tweets_chunks)
                appended_tweets = ""
    return appended_tweets_chunks


# the translations are done by text paragraphs composed by multiple tweets,, to avoid exceeding the request limits.
def translate_tweets_chunks(appended_tweets_chunks):
    translated_chunks = []
    tweets_eng = []
    for chunk in appended_tweets_chunks:
        # print("*********")
        # print (chunk)
        # print("going to translate")
        translation = translate_tweets(chunk)
        # print("translation done")
        # print(translation)
        translated_chunks.append(translation)

        for translation in translated_chunks:
            translated_tweets = translation.split(";;")
            for trans in translated_tweets:
                tweets_eng.append(trans)
    return tweets_eng


# translation process: it takes a string of concatenated individual tweets to avoid translation limits
def translate_tweets(tweets):
    translation = ""
    try:
        translator = Translator()
        translation = translator.translate(tweets, src='auto', dest='en')
        translation = translation.text
    except:
        pass
    return (translation)


# main translation function to translate users text for analysis tools
def translate_all_user_tweets(tweets):
    tweets_en = []
    appended_tweets_chunks = build_query_chunks(tweets)
    print("appended_tweets_chunks")
    print(appended_tweets_chunks)
    # translate texts
    # print("NEW CHUNK")
    # print("----",appended_tweets_chunks)
    tweets_en = translate_tweets_chunks(appended_tweets_chunks)

    return (tweets_en)


# function called for the translation of all spanish and catalan users
def get_translated_tweets(users_data_df, lang):
    users_translated = {}
    for index, row in users_data_df.iterrows():
        # rint(index)
        if lang == 'es' or lang == 'ca':
            user_tweets = get_texts(row['tweets'])

            print("user_tweets")
            print(user_tweets)

            user_english_tweets = translate_all_user_tweets(user_tweets)
            print("user_english_tweets")
            print(user_english_tweets)
            del user_english_tweets[-1]
            users_translated[row['user_id']] = user_english_tweets
            # store translated users in case there are connectivity errors
            with open('translated_esp_cat_to_eng_user.json', 'w', encoding='utf8') as json_file:
                json.dump(users_translated, json_file, ensure_ascii=True)

    return users_translated


# TOPICS

def calculate_users_topics(users_data_df, lang, users_translated):
    users_topics = {}
    topics_results = {}
    print("Before Download")
    model = tweetnlp.load_model('topic_classification')
    print("After Download")

    for index, row in users_data_df.iterrows():
        if row['user_id'] not in topics_results:
            if lang == 'es' or lang == 'ca':
                if len(users_translated[row['user_id']]) >= 400:
                    user_tweets = random.sample(users_translated[row['user_id']], 400)
                else:
                    user_tweets = users_translated[row['user_id']]
            else:
                texts = get_texts(row['tweets'])
                if len(texts) >= 400:
                    user_tweets = random.sample(texts, 400)
                else:
                    user_tweets = texts

            users_topics[row['user_id']] = []

            for tweet in user_tweets:
                if len(tweet.split(" ")) > 3:
                    tweet_results = model.topic(tweet, return_probability=True)['probability']
                    users_topics[row['user_id']].append(tweet_results)

            topics_results[row['user_id']] = calculate_topics_averages(users_topics[row['user_id']])
            print("save Start1 =", datetime.now().strftime("%H:%M:%S"))
            out_file = open("english_topics_data_tweetnlp_user.json", "w")
            json.dump(topics_results, out_file)
            out_file.close()
            print("save End1 =", datetime.now().strftime("%H:%M:%S"))
            # print(topics_results)
    print("topics_results ", topics_results)
    return topics_results


def calculate_topics_averages(user_topics):
    topics_aggregations = {}
    for tweet_topics in user_topics:
        for topic in tweet_topics:
            # print(topic)
            if topic not in topics_aggregations:
                topics_aggregations[topic] = 0
                # print(tweet_topics[topic])
            topics_aggregations[topic] += float(tweet_topics[topic])
    for topic in topics_aggregations:
        topics_aggregations[topic] = topics_aggregations[topic] / len(user_topics)
    # print(topics_aggregations)
    return topics_aggregations


# 2. Get topics addressed using empath applied over getting 400 random tweets per user
lexicon = Empath()


def calculate_empath_topics(users_data_df, lang, users_translated):
    users_topics = {}
    topics_results = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in topics_results:
            # rint(row['user_id'])

            if lang == 'es' or lang == 'ca':
                if len(users_translated[row['user_id']]) >= 400:
                    user_tweets = random.sample(users_translated[row['user_id']], 400)
                else:
                    user_tweets = users_translated[row['user_id']]
            else:
                texts = get_texts(row['tweets'])
                if len(texts) >= 400:
                    user_tweets = random.sample(texts, 400)
                else:
                    user_tweets = texts

            users_topics[row['user_id']] = []
            for tweet in user_tweets:
                if len(tweet.split(" ")) > 3:
                    # print(tweet)
                    tweet_results = lexicon.analyze(tweet, normalize=True)
                    users_topics[row['user_id']].append(tweet_results)
            topics_results[row['user_id']] = calculate_topics_averages(users_topics[row['user_id']])
            print("save Start2 =", datetime.now().strftime("%H:%M:%S"))
            out_file = open("english_topics_data_empaths_user.json", "w")
            print("save End2 =", datetime.now().strftime("%H:%M:%S"))
            json.dump(topics_results, out_file)
            out_file.close()
        # print(topics_results)

    return topics_results


# fuction that adds to the original data dict the new topic features
def add_new_features(features_calculated, data, library):
    unique_features = []
    pos = 0
    for element in data:
        # print ("Element--",element)
        if element['user_id'] in features_calculated:
            for feature in features_calculated[element['user_id']]:
                data[pos][feature + "_" + library] = features_calculated[element['user_id']][feature]
                if (feature + "_" + library) not in unique_features:
                    unique_features.append(feature + "_" + library)
        pos += 1
    return unique_features


# EMOTIONS

# load emolex dictionary
def load_emolex(lang):
    if lang == 'es':
        lexicon = EmoLex('emo-lex-spanishs.txt')
    else:
        lexicon = EmoLex('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')
    return lexicon


# calculate score per tweet
def calculate_emolex_score(tweet, lexicon):
    # tokenize tweet
    document = nltk.tokenize.casual.casual_tokenize(tweet)
    # print(document)
    summary = lexicon.summarize_doc(document)
    return summary


# calculate emolex scores for all users
def calculate_emolex_features(users_data_df, lang, users_translated):
    span_lex = load_emolex('es')

    eng_lex = load_emolex('en')

    users_emotions = {}
    emotions_results = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in emotions_results:
            # print(row['user_id'])

            # catalan texts should use the english translation
            if lang == 'ca':
                texts = users_translated[row['user_id']]
            else:
                texts = get_texts(row['tweets'])
            users_emotions[row['user_id']] = []
            for tweet in texts:
                # spanish
                if lang == "es":
                    tweet_results = calculate_emolex_score(tweet, span_lex)
                    # english
                else:
                    tweet_results = calculate_emolex_score(tweet, eng_lex)
                users_emotions[row['user_id']].append(tweet_results)
            emotions_results[row['user_id']] = calculate_topics_averages(users_emotions[row['user_id']])
            print("save Start3 =", datetime.now().strftime("%H:%M:%S"))
            out_file = open("emolex_emotions_user.json", "w")
            json.dump(emotions_results, out_file)
            out_file.close()
            print("save End3 =", datetime.now().strftime("%H:%M:%S"))
        # print(topics_results)

    return emotions_results


# SENTIMENT
# calculate emolex scores for all users

def calculate_sentiment_features(users_data_df, lang, users_translated):
    model = tweetnlp.load_model('sentiment',
                                multilingual=True)  # Or `model = tweetnlp.Sentiment(multilingual=True)`
    users_sents = {}
    sents_results = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in sents_results:
            # print(row['user_id'])

            # catalan texts should use the english translation
            if lang == 'ca':
                texts = users_translated[row['user_id']]
                if len(texts) >= 200:
                    texts = random.sample(texts, 200)
            else:
                texts = get_texts(row['tweets'])
                if len(texts) >= 200:
                    texts = random.sample(texts, 200)

            users_sents[row['user_id']] = []
            for tweet in texts:
                tweet_results = model.sentiment(tweet, return_probability=True)["probability"]
                users_sents[row['user_id']].append(tweet_results)
            sents_results[row['user_id']] = calculate_topics_averages(users_sents[row['user_id']])
            print("save Start4 =", datetime.now().strftime("%H:%M:%S"))
            out_file = open("sentiment_results_user.json", "w")
            json.dump(sents_results, out_file)
            out_file.close()
            print("save End4 =", datetime.now().strftime("%H:%M:%S"))
        # print(topics_results)

    return sents_results


# HATE SPEECH
def calculate_hate_speech_features(users_data_df, lang, users_translated):
    model = tweetnlp.load_model('hate')  # Or `model = tweetnlp.Sentiment(multilingual=True)`    
    users_hatesp = {}
    hatesp_results = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in hatesp_results:
            # print(row['user_id'])

            # catalan texts should use the english translation
            if lang == 'es' or lang == 'ca':
                texts = users_translated[row['user_id']]
                if len(texts) >= 200:
                    texts = random.sample(texts, 200)
            else:
                texts = get_texts(row['tweets'])
                if len(texts) >= 200:
                    texts = random.sample(texts, 200)

            users_hatesp[row['user_id']] = []
            for tweet in texts:
                tweet_results = model.hate(tweet, return_probability=True)["probability"]
                users_hatesp[row['user_id']].append(tweet_results)
            hatesp_results[row['user_id']] = calculate_topics_averages(users_hatesp[row['user_id']])
            print("save Start5 =", datetime.now().strftime("%H:%M:%S"))
            out_file = open("hate_speech_results_user.json", "w")
            json.dump(hatesp_results, out_file)
            out_file.close()
            print("save End5 =", datetime.now().strftime("%H:%M:%S"))
        # print(topics_results)

    return hatesp_results


# LIWC
def generate_ngrams(s, n):
    # Convert to lowercases
    s = s.lower()

    # Replace all none alphanumeric characters with spaces
    s = re.sub(r'[^a-zA-Z0-9\s]', ' ', s)

    # Break sentence in the token, remove empty tokens
    tokens = [token for token in s.split(" ") if token != ""]

    # Use the zip function to help us generate n-grams
    # Concatentate the tokens into ngrams and return
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return ["_".join(ngram) for ngram in ngrams]


# calculate liwc features
def calculate_LIWC_features(users_data_df, lang, users_translated):
    users_liwc_scores = {}
    users_results = {}
    parse_sp, category_names_sp = liwc.load_token_parser('LIWC2007_Spanish_utf.dic')
    parse_en, category_names_en = liwc.load_token_parser('LIWC2007_English.dic')
    all_existent_categories = []
    # por el texto de cada usuario
    for index, row in users_data_df.iterrows():

        if row['user_id'] not in users_results:
            # print(row['user_id'])

            # catalan texts should use the english translation
            if lang == 'ca':
                texts = users_translated[row['user_id']]
            else:
                texts = get_texts(row['tweets'])
            users_liwc_scores[row['user_id']] = []
            for tweet in texts:
                text_size = len(tweet.split(" "))
                user_text_tokens = generate_ngrams(tweet, 1)
                user_text_bigrams = generate_ngrams(tweet, 2)
                user_text_trigrams = generate_ngrams(tweet, 3)
                user_text_4grams = generate_ngrams(tweet, 4)
                for element in user_text_bigrams:
                    user_text_tokens.append(element)
                for element in user_text_trigrams:
                    user_text_tokens.append(element)
                for element in user_text_4grams:
                    user_text_tokens.append(element)
                # spanish
                if lang == 'es':
                    # obtener conteo de palabras del diccionario por c/usuario
                    liwc_categories_count = Counter(
                        category for token in user_text_tokens for category in parse_sp(token))
                # english
                else:
                    liwc_categories_count = Counter(
                        category for token in user_text_tokens for category in parse_en(token))
                liwc_cats = {}
                for key in liwc_categories_count:
                    liwc_cats[key] = float(liwc_categories_count[key]) / float(text_size)
                users_liwc_scores[row['user_id']].append(liwc_cats)
            users_results[row['user_id']] = calculate_topics_averages(users_liwc_scores[row['user_id']])
            print("save Start6 =", datetime.now().strftime("%H:%M:%S"))
            out_file = open("liwc_scores_user.json", "w")
            json.dump(users_results, out_file)
            out_file.close()
            print("save End6 =", datetime.now().strftime("%H:%M:%S"))

    # print(topics_results)

    return users_results


# GENDER AND AGE
# To use this functions you must have a twitter account (API v1) and provide your credentials, also the versions of torch described are required

# infer age and gender
def get_users_gender_age(users_data_df, consumer_key, consumer_secret, access_token, access_token_secret):
    print("1")
    m3twitter = M3Twitter()
    print("2")
    m3twitter.twitter_init(api_key=consumer_key, api_secret=consumer_secret, access_token=access_token,
                           access_secret=access_token_secret)
    print("3")
    users_info = {}
    sents_results = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in sents_results:
            # print(row['user_id'])
            users_info[row['user_id']] = m3twitter.infer_id(row['twitter_id'])['output']
    print("5")
    return users_info


# summarize results
def process_results(users_info):
    users_demog = {}
    ages_map = ['<=18', '19-29', '30-39', '>=40']
    for user in users_info:
        # print (user)
        users_demog[user] = {}
        if 'org' in users_info[user]:
            if users_info[user]['org']['is-org'] > 0.90:
                users_demog[user]['gender'] = 'organization'  # organization
                users_demog[user]['age'] = 'organization'  # age for the case of a organization
            else:
                if 'gender' in users_info[user]:
                    if users_info[user]['gender']['male'] > users_info[user]['gender']['female']:
                        users_demog[user]['gender'] = 'male'  # male
                    else:
                        users_demog[user]['gender'] = 'female'  # female
                    user_age = []
                    user_age.append(users_info[user]['age']['<=18'])  # label 0
                    user_age.append(users_info[user]['age']['19-29'])  # label 1
                    user_age.append(users_info[user]['age']['30-39'])  # label 2
                    user_age.append(users_info[user]['age']['>=40'])  # label 3
                    label = user_age.index(max(user_age))
                    users_demog[user]['age'] = ages_map[label]

    return users_demog


# SOCIAL SUPPORT
def get_tweets_rts_likes_mentions_counts(users_data_df):
    tweets_vals_all = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in tweets_vals_all:
            # print(row['user_id'])
            tweets_vals = {}
            tweet_sizes = 0
            retweets = 0
            favs = 0
            mentions = 0
            tweets_count = 0
            for tweet in row['tweets']:
                if tweet['is_retweet'] == 'false':  # ver soporte recibido solo a tweets propios
                    retweets += int(tweet['retweet_count'])
                    favs += int(tweet['favorite_count'])
                    mentions += int(tweet['users_mentions_count'])
                    tweets_count += 1
            if tweets_count > 0:
                tweets_vals['retweets_count'] = (retweets / tweets_count)
                tweets_vals['favs_count'] = (favs / tweets_count)
                tweets_vals['mentions_count'] = (mentions / tweets_count)
            else:
                tweets_vals['retweets_count'] = 0
                tweets_vals['favs_count'] = 0
                tweets_vals['mentions_count'] = 0
            tweets_vals_all[row['user_id']] = tweets_vals
    print(tweets_vals_all)
    return tweets_vals_all


# GET TWEETS TYPES
def get_tweets_types_count(users_data_df):
    tweets_types_counts = {}
    for index, row in users_data_df.iterrows():
        if row['user_id'] not in tweets_types_counts:
            # print(row['user_id'])
            tweets_vals = {}
            quotes = 0
            retweets = 0
            replies = 0
            original = 0

            for tweet in row['tweets']:
                if tweet['is_quoted'] == 'true':
                    quotes += 1
                if tweet['is_retweet'] == 'true':
                    retweets += 1
                if tweet['is_reply'] == 'true':
                    replies += 1
                if tweet['is_quoted'] == 'false' and tweet['is_retweet'] == 'false' and tweet['is_reply'] == 'false':
                    original += 1
            tweets_vals['ratio_quoted'] = (quotes / len(row['tweets']))
            tweets_vals['ratio_retweets'] = (retweets / len(row['tweets']))
            tweets_vals['ratio_replies'] = (replies / len(row['tweets']))
            tweets_vals['ratio_original'] = (original / len(row['tweets']))
            tweets_types_counts[row['user_id']] = tweets_vals
    return tweets_types_counts


# BEHAVIORAL FEATURES

def get_list_tweet_creation_dates(users_data_df):
    users_created_at = {}
    for index, row in users_data_df.iterrows():
        users_created_at[row['user_id']] = []
        for tweet in row['tweets']:
            users_created_at[row['user_id']].append(tweet['created_at'])
    return users_created_at


def calculate_median(numbers_list):
    if len(numbers_list) > 1:
        median_count = stat.median(numbers_list)

    elif len(numbers_list) == 1:
        median_count = numbers_list[0]
    else:
        median_count = 0

    return (median_count)


def calculate_avg(numbers_list):
    if len(numbers_list) > 1:
        mean_count = stat.mean(numbers_list)

    elif len(numbers_list) == 1:
        mean_count = numbers_list[0]
    else:
        mean_count = 0
    return (mean_count)


# calculate sleep_time and awake_time features

# function to calculate the ratio of tweets per day and weekend
def count_tweets_per_day(timestamp_list):
    week_days_count = 0
    weekend_days_count = 0

    for timestamp in timestamp_list:
        timestamp_day = timestamp.split(" ")[0]

        if ('Sat' in timestamp_day) or ('Sun' in timestamp_day):
            weekend_days_count += 1
        else:
            week_days_count += 1

    week_days_count_ratio = week_days_count / len(timestamp_list)
    weekend_days_count_ratio = weekend_days_count / len(timestamp_list)
    return week_days_count, weekend_days_count, week_days_count_ratio, weekend_days_count_ratio


# fuction to calculate the average/median of the time diference between tweets
def calculate_avg_median_time_between_tweets(timestamp_list):
    pos_i = 0
    pos_j = 1
    ordered_timestamp_list = []
    for element in timestamp_list:
        # print(element)
        # converted_date=datetime.strptime(element,'%a %b %d %H:%M:%S %z %Y')
        converted_date = datetime.strptime(element, '%Y-%m-%dT%H:%M:%S.%fZ')
        # rint (converted_date)
        ordered_timestamp_list.append(converted_date)
    # sort elements by date
    ordered_timestamp_list.sort()
    time_differences_list = []

    while pos_j < len(ordered_timestamp_list):
        temporal_j_val = ordered_timestamp_list[pos_j]
        temporal_i_val = ordered_timestamp_list[pos_i]

        # print(temporal_j_val)
        # print(temporal_i_val)
        time_delta = temporal_j_val - temporal_i_val
        # print(time_delta)
        totsec = time_delta.total_seconds()
        # print(totsec)
        if totsec != 0:
            time_differences_list.append(totsec)
        pos_i += 1
        pos_j += 1
    # print(time_differences_list)
    avg_time_bet_tweets = calculate_avg(time_differences_list)
    median_time_bet_tweets = calculate_median(time_differences_list)
    return avg_time_bet_tweets, median_time_bet_tweets, ordered_timestamp_list


def calculate_sleep_time_awake_time_features(ordered_timestamp_list):
    timeframes_count = [0, 0, 0, 0, 0, 0, 0, 0]  # store counts for each time frame
    sum_continuos_time_slots = []
    timeframes_count_normalized = []
    reference_times = []  # store time reference points
    counter = 0
    reference_time_hours = datetime.strptime("Wed Jul 17 00:00:00 +0000 2019",
                                             '%a %b %d %H:%M:%S %z %Y').time()  # 00:00 initial reference date
    reference_time = datetime.strptime("Wed Jul 17 00:00:00 +0000 2019", '%a %b %d %H:%M:%S %z %Y')
    reference_end_time = datetime.strptime("Wed Jul 17 23:59:59 +0000 2019",
                                           '%a %b %d %H:%M:%S %z %Y').time()  # 23:59:59 last reference date
    # print(reference_time)
    reference_times.append(reference_time_hours)  # append first reference time

    while counter < 7:  # add reference times each 3 hours
        reference_time = reference_time + timedelta(minutes=180)
        reference_time_hours = reference_time.time()
        reference_times.append(reference_time_hours)
        counter += 1

    reference_times.append(reference_end_time)  # append the reference time

    for element in ordered_timestamp_list:  # check where each time falls within the 8 time periods defined
        element = element.time()
        # print(element)
        timeframes_counter = 0
        while timeframes_counter < 8:
            # print(element,reference_times[timeframes_counter] )
            if (element >= reference_times[timeframes_counter]):
                if (element < reference_times[timeframes_counter + 1]):
                    timeframes_count[timeframes_counter] += 1
                    # print(element,reference_times[timeframes_counter], reference_times[timeframes_counter+1])
                    # print(element, timeframes_count)
            timeframes_counter += 1
    for element in (timeframes_count):
        element = element / (len(ordered_timestamp_list))
        timeframes_count_normalized.append(element)

    # calculate sleep and awake tweets sum
    slots_counter = 0
    for slot in timeframes_count:
        if slots_counter == (len(timeframes_count) - 1):
            sum_continuos_time_slots.append(slot + timeframes_count[0])
        else:
            next_slot = timeframes_count[slots_counter + 1]
            sum_continuos_time_slots.append(slot + next_slot)
        slots_counter += 1

    tweets_sleep_time_number = min(sum_continuos_time_slots)
    normalized_sleep_time_tweets = tweets_sleep_time_number / (len(ordered_timestamp_list))
    # print(sum_continuos_time_slots, tweets_sleep_time_number, normalized_sleep_time_tweets)
    normalized_awake_time_tweets = 1 - normalized_sleep_time_tweets

    # print(timeframes_count)
    # print(timeframes_count_normalized)
    # print (reference_time)
    return normalized_sleep_time_tweets, normalized_awake_time_tweets


def calculate_behavioral_features(users_tweets_dates):
    behavioral_feats = {}
    for user in users_tweets_dates:
        week_days_count, weekend_days_count, week_days_count_ratio, weekend_days_count_ratio = count_tweets_per_day(
            users_tweets_dates[user])
        behavioral_feats[user] = {}
        behavioral_feats[user]["week_days_count_ratio"] = week_days_count_ratio
        # print(week_days_count_ratio)
        behavioral_feats[user]["weekend_days_count_ratio"] = weekend_days_count_ratio
        # print(weekend_days_count_ratio)
        avg_time_bet_tweets, median_time_bet_tweets, ordered_timestamp_list = calculate_avg_median_time_between_tweets(
            users_tweets_dates[user])
        behavioral_feats[user]["median_time_betweet_tweets"] = median_time_bet_tweets
        # tweets sleep time awake time
        tweets_sleep_time_ratio, tweets_awake_time_ratio = calculate_sleep_time_awake_time_features(
            ordered_timestamp_list)
        behavioral_feats[user]["tweets_sleep_time_ratio"] = tweets_sleep_time_ratio
        behavioral_feats[user]["tweets_awake_time_ratio"] = tweets_awake_time_ratio
    return behavioral_feats


def generate_user_features(user_data_df, lang, consumer_key, consumer_secret, access_token, access_token_secret,
                           user_data):
    # translate tweets to english
    print("Translate tweets =", datetime.now().strftime("%H:%M:%S"))
    tweets_translated = get_translated_tweets(user_data_df, lang)

    # calculate users topics - using tweetnlp
    # print("Calculate Users Topics =", datetime.now().strftime("%H:%M:%S"))
    # topics_tweetnlp= calculate_users_topics(user_data_df, lang,tweets_translated)
    # print ("topics_tweetnlp", topics_tweetnlp)
    # print("AddFeatures =", datetime.now().strftime("%H:%M:%S"))
    # tweetnlp_topics_names=add_new_features(topics_tweetnlp,user_data,'tweetnlp')

    # calculate users topics - using empath
    print("Calculate Empath Topics=", datetime.now().strftime("%H:%M:%S"))
    topics_empath = calculate_empath_topics(user_data_df, lang, tweets_translated);
    empath_topics_names = add_new_features(topics_empath, user_data, 'empath');
    # calculate user emotions
    print("Calculate Emolex =", datetime.now().strftime("%H:%M:%S"))
    emotions = calculate_emolex_features(user_data_df, lang, tweets_translated);
    emotions_names = add_new_features(emotions, user_data, "emolex");
    # calculate sentiment analysis
    print("Calculate Sentiment Analysis =", datetime.now().strftime("%H:%M:%S"))
    sentiment_features = calculate_sentiment_features(user_data_df, lang, tweets_translated);
    sentiment_names = add_new_features(sentiment_features, user_data, 'sentiment');
    # calculate hate speech features
    print("Calculate Hate Speech =", datetime.now().strftime("%H:%M:%S"))
    hate_speech_feats = calculate_hate_speech_features(user_data_df, lang, tweets_translated);
    hate_speech_names = add_new_features(hate_speech_feats, user_data, "hate_sp");
    # liwc attributes
    print("Calculate LIWC =", datetime.now().strftime("%H:%M:%S"))
    liwc_feats = calculate_LIWC_features(user_data_df, lang, tweets_translated);
    print(liwc_feats)
    liwc_names = add_new_features(liwc_feats, user_data, "liwc");
    print(liwc_feats)
    # gender and age attributes
    print("Calculate users gender age =", datetime.now().strftime("%H:%M:%S"))
    gender_age_feats = get_users_gender_age(user_data_df, consumer_key, consumer_secret, access_token,
                                            access_token_secret);
    demog_feats = process_results(gender_age_feats);
    age_gender_names = add_new_features(demog_feats, user_data, "demo");
    # social_support
    print("Calculate rtslikesmentionscounts =", datetime.now().strftime("%H:%M:%S"))
    social_feats = get_tweets_rts_likes_mentions_counts(user_data_df);
    social_feats_names = add_new_features(social_feats, user_data, "social");
    # tweet types
    print("Calculate get_tweets_types_count =", datetime.now().strftime("%H:%M:%S"))
    tweet_types_feats = get_tweets_types_count(user_data_df);
    tweets_types_names = add_new_features(tweet_types_feats, user_data, "tweet_types");
    # Behavioral features
    print("Calculate get_list_tweet_creation_dates =", datetime.now().strftime("%H:%M:%S"))
    creation_dates = get_list_tweet_creation_dates(user_data_df);
    behavioral_features = calculate_behavioral_features(creation_dates);
    behavioral_feats = add_new_features(behavioral_features, user_data, "behav");
    return user_data


def load_relevant_features(relevant_features_path):
    relevant_features = {}
    with open(relevant_features_path, 'r', encoding='utf-8') as f:
        content = f.readlines()
        for line in content:
            feature = line.replace("\n", "")
            if ":" in feature:
                relevant_features[feature] = []
                current_feat = feature
            else:
                relevant_features[current_feat].append(feature)
    return (relevant_features)


# load median values
def load_medians_file(filepath, relevant_feats):
    relevant_vals = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.readlines()
        for line in content:
            line = line.split("\t")
            feature = line[0]
            median = line[1].replace("\n", "")
            for feature_type in relevant_feats:
                if feature in relevant_feats[feature_type]:
                    if feature_type not in relevant_vals:
                        relevant_vals[feature_type] = {}
                    relevant_vals[feature_type][feature] = median
    return relevant_vals


def load_medians_file_all_features(filepath):
    relevant_vals = {}
    relevant_vals['AllValues'] = {}
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.readlines()
        for line in content:
            line = line.split("\t")
            feature = line[0]
            median = line[1].replace("\n", "")
            relevant_vals['AllValues'][feature] = median
    return relevant_vals


def change_categorical_features(user_features):
    # print ("UF",user_features)
    print("TYPE IS.....", type(user_features))
    # print ("UF0",user_features[0])
    new_feats = dict(user_features[0])
    for feature in new_feats:
        # age features
        # print ("Features is", feature)
        if feature == "age_demo":
            user_features[0]["age_demo_19-29"] = 1
            user_features[0]["age_demo_<=18"] = 1
            user_features[0]["age_demo_30-39"] = 1
            user_features[0]["age_demo_>=40"] = 1
            user_features[0]["age_demo_organization"] = 0
            if user_features[0][feature] == '19-29':
                user_features[0]["age_demo_19-29"] = 1
            elif user_features[0][feature] == '<=18':
                user_features[0]["age_demo_<=18"] = 1
            elif user_features[0][feature] == '30-39':
                user_features[0]["age_demo_30-39"] = 1
            elif user_features[0][feature] == '>=40':
                user_features[0]["age_demo_>=40"] = 1
            else:
                user_features[0]["age_demo_organization"] = 1

        # gender features
        if feature == "gender_demo":
            if user_features[0][feature] == 'female':
                user_features[0]["gender_demo_female"] = 1
                user_features[0]["gender_demo_organization"] = 0
            elif user_features[0][feature] == 'male':
                user_features[0]["gender_demo_male"] = 1
                user_features[0]["gender_demo_organization"] = 0
            else:
                user_features[0]["gender_demo_organization"] = 1

        # url in profile
        if feature == "url_in_profile":
            user_features[0]["url_in_profile_0"] = 0
            user_features[0]["url_in_profile_1"] = 0
            if user_features[0][feature] == 1:
                user_features[0]["url_in_profile_1"] = 1
            else:
                user_features[0]["url_in_profile_0"] = 1

                # is verified
        if feature == "is_verified":
            user_features[0]["is_verified_False"] = 0
            user_features[0]["is_verified_True"] = 0
            if user_features[0][feature] == 1:
                user_features[0]["is_verified_True"] = 1
            else:
                user_features[0]["is_verified_False"] = 1
    return user_features


def get_median_values_users(lang):
    if lang == 'en':
        rel_feats = load_relevant_features("results_selected_features_names1_fake_spreaders_en3_random_en")
        rel_fake_spreaders = load_medians_file("results_1_fake_spreaders_en", rel_feats)
        rel_fact_checkers = load_medians_file("results_2_fact_checkers_eng", rel_feats)
        rel_random = load_medians_file("results_3_random_en", rel_feats)
    elif lang == 'es':
        rel_feats = load_relevant_features("results_selected_features_names4_fake_spreaders_esp6_random_es")
        rel_fake_spreaders = load_medians_file("results_4_fake_spreaders_esp", rel_feats)
        rel_fact_checkers = load_medians_file("results_5_fact_checkers_esp", rel_feats)
        rel_random = load_medians_file("results_6_random_es", rel_feats)
    else:  # asumiendo catalan sin que existan otros lenguajes
        rel_feats = load_relevant_features("results_selected_features_names6_random_es7_fake_spreaders_cat")
        rel_fake_spreaders = load_medians_file("results_7_fake_spreaders_cat", rel_feats)
        rel_fact_checkers = load_medians_file("results_5_fact_checkers_esp", rel_feats)
        rel_random = load_medians_file("results_6_random_es", rel_feats)
    return rel_feats, rel_fake_spreaders, rel_fact_checkers, rel_random


def get_all_values_users(lang):
    if lang == 'en':
        rel_fake_spreaders = load_medians_file_all_features("results_1_fake_spreaders_en")
        rel_fact_checkers = load_medians_file_all_features("results_2_fact_checkers_eng")
        rel_random = load_medians_file_all_features("results_3_random_en")
    elif lang == 'es':
        rel_fake_spreaders = load_medians_file_all_features("results_4_fake_spreaders_esp")
        rel_fact_checkers = load_medians_file_all_features("results_5_fact_checkers_esp")
        rel_random = load_medians_file_all_features("results_6_random_es")
    else:  # asumiendo catalan sin que existan otros lenguajes
        rel_fake_spreaders = load_medians_file_all_features("results_7_fake_spreaders_cat")
        rel_fact_checkers = load_medians_file_all_features("results_5_fact_checkers_esp")
        rel_random = load_medians_file_all_features("results_6_random_es")
    return rel_fake_spreaders, rel_fact_checkers, rel_random


def convertDictToDataframe(relevantFeaturesDict):
    # df = pd.DataFrame();
    listOfKeys = [];
    listOfValues = []
    for dictTittle, currentDict in relevantFeaturesDict.items():
        # print("\-------------------------DIC TITTLE:", dictTittle)
        for key in currentDict.keys():
            # print("	",key + ':', currentDict[key])
            listOfKeys.append(key);
            listOfValues.append(currentDict[key])
    # df[str(key)]=currentDict[key];
    #	print ("DataFrame is ", pd)
    # print ("keys", listOfKeys)
    #	print ("values", listOfValues)
    #	print("lengths", len(listOfKeys),len(listOfValues));
    tmpDict = dict(zip(listOfKeys, listOfValues))
    df = pd.DataFrame(tmpDict, index=[0]);
    # print("    /-/*-/*-/*-/-*/*-/-*/-*/-*/-*/-DATAFRAME IS",df.to_string());
    # print ("Size is ", df.info())
    return df


def get_medians_dfs(all_features_medians, user_features):
    user_selected_features = {}
    all_selected_feats_meds = {}
    all_selected_feats_groups = []
    for feature_type in all_features_medians[0]:
        user_selected_features[feature_type] = {}
        for element in all_features_medians[0][feature_type]:
            if element in user_features[0]:
                # print("Element",element)
                user_selected_features[feature_type][element] = user_features[0][element]
    # print(user_selected_features)
    all_features_medians.append(user_selected_features)
    classes_medians = {}
    classes_medians_dfs = {}
    counter = 0
    for group in all_features_medians:
        classes_medians[counter] = {}
        for feature_type in all_features_medians[0]:
            if feature_type not in classes_medians[counter]:
                classes_medians[counter][feature_type] = []
            classes_medians[counter][feature_type].append(all_features_medians[counter][feature_type])
        counter += 1
    groups_feats = []
    # print(classes_medians)
    for group in classes_medians:
        # print("Group", group)
        group_feat = {}
        for feature_type in classes_medians[group]:
            # print("FT",feature_type)
            for key in classes_medians[group][feature_type][0]:
                # print("K",key)
                val = 0
                # print("classes_medians[group][feature_type][0][key] TYPE",type(classes_medians[group][feature_type][0][key]))
                # print("is dict", isinstance(classes_medians[group][feature_type][0][key],dict))
                # print("classes_medians[group][feature_type][0][key]",classes_medians[group][feature_type][0][key])
                if (isinstance(classes_medians[group][feature_type][0][key], dict)):
                    # print ("it is dict")
                    currentSubKeys = list(classes_medians[group][feature_type][0][key].keys())
                    # print ("csk", currentSubKeys[0])
                    val = classes_medians[group][feature_type][0][key][currentSubKeys[0]]
                else:
                    val = classes_medians[group][feature_type][0][key]
                group_feat[key] = val
        groups_feats.append(group_feat)

    for group in classes_medians:
        classes_medians_dfs[group] = pd.DataFrame(classes_medians[group])

    all_selected_feats_meds_df = pd.DataFrame(groups_feats)

    return classes_medians_dfs, all_selected_feats_meds_df


# Define function to calculate cosine similarity between last row and all remaining rows
def cosine_similarity_last_row(df):
    last_row = df.iloc[-1]  # Select last row of dataframe
    remaining_rows = df.iloc[:-1]  # Select all remaining rows of dataframe
    similarity_values = remaining_rows.apply(
        lambda row: np.dot(row, last_row) / (np.sqrt(np.sum(row ** 2)) * np.sqrt(np.sum(last_row ** 2))), axis=1)

    totalSum = similarity_values[0] + similarity_values[1] + similarity_values[2];
    similarity_values[0] = similarity_values[0] / totalSum;
    similarity_values[1] = similarity_values[1] / totalSum;
    similarity_values[2] = similarity_values[2] / totalSum;

    return similarity_values


def initTwitterLibraries():
    # API calls
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    api = tweepy.API(auth, wait_on_rate_limit=True)
    return api


# Upon this point we put the functions to use the openai interface.
def add_text_separator(text):
    text = "Tweet: " + text.replace("\n", "").replace("\t", "").strip()
    return text


def paraphrase_text(text, lang):
    # Cambia segÃºn el lenguaje del texto, para online se debe adaptar y tomar el lenguaje como argumentos
    catStr = 'ca'
    castStr = 'es'
    angStr = 'en'

    # print ("lang is ", lang)
    if (lang == catStr):
        search_term = "Sempre mantenint el to, nomÃ©s parafraseja els textos segÃ¼ents. La resposta ha de tenir un text (tweet) per fila: " + text;
    elif (lang == castStr):
        search_term = "Ãºnicamente parafrasea los siguientes textos para anonimizarlos: La respuesta tiene que tener un text (tweet) por fila: " + text;
    elif (lang == angStr):
        search_term = "just paraphrase the following tweets (each text is separated by a tab): " + text;

    # print(search_term)de cada usuario en su respectivo lenguaje original. Cada texto estÃ¡ separado por una tabulaciÃ³n, l
    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": search_term}
        ]
    )
    result = completion.choices[0].message.content
    # print(result)
    return result


def paraphrase_tweets(df_data, lang):
    # print ("Hola df_data", df_data.to_string())
    df_data['Tweets'] = df_data['Tweets'].apply(add_text_separator)
    # print(df_data['Tweets'])
    concatenated_text = df_data['Tweets'].str.cat(sep='\t')
    # print(concatenated_text)
    # print(len(concatenated_text.split('\t')))
    # print(len(concatenated_text.split('\t')))
    paraphrased = paraphrase_text(concatenated_text, lang)
    paraphrased = paraphrased.replace("Usuario ", "").split('\n')

    paraphrased = list(filter(None, paraphrased))
    # print ("Len paraphrased is ", len(paraphrased))
    #	if len(paraphrased)!=10:
    #		print(len(paraphrased))
    #		return df_data
    df_data['anonymized_text'] = paraphrased;  # to remove the first word Tweet
    return df_data


# Specify the path of the parent folder
# esta funciÃ³n es creada para el caso offline donde proceso todas las carpetas de los temas definidos
def paraphrase_individual_tweets(parent_folder_path):
    # Iterate over each folder within the parent folder
    counter = 0
    for folder_name in os.listdir(parent_folder_path):
        # Construct the full path of the folder
        folder_path = os.path.join(parent_folder_path, folder_name)

        # Check if the item is a folder
        if os.path.isdir(folder_path):
            # Process the folder
            print(f"Processing folder: {folder_name}")

            # Iterate over each file within the folder
            for file_name in os.listdir(folder_path):

                if ("_count_rank_display_df" in file_name) or (
                        "_popularity_collected_tweets_rank_display_df" in file_name) or (
                        "_popularity_tweets_rank_display_df" in file_name):
                    time.sleep(30)
                    file_path = os.path.join(folder_path, file_name)
                    df_rank = pd.read_json(file_path, orient='records', lines=True, convert_dates=['Date Created'])
                    df_rank = paraphrase_tweets(df_rank)
                    df_rank.to_json(file_path.replace('.json', '') + '_anonymized.json', orient='records', lines=True)

                    counter += 1
    # print(counter)


# load_dataframes(parent_folder_path)
# paraphrase_individual_tweets("add_path_here")

def gpt_request_description(text):
    # time.sleep(60)
    # search_term="Ãºnicamente parafrasea los siguientes textos para anonimizarlos: "+text
    # gpt_instruction="make a brief description (max: 50 words) of the following twitter user considering its profile description (first text) and a sample of its tweets (remaining texts): "+text
    # gpt_instruction="haz una breve descripciÃ³n (max: 50 palabras) del siguiente usuario de twitter considerando su descripciÃ³n (primer texto) y una muestra de sus tweets (textos restantes): "+text
    gpt_instruction = "fes una breu descripciÃ³ (max: 50 paraules) del segÃ¼ent usuari de twitter considerant la seva descripciÃ³ (primer text) i una mostra dels seus tweets (textos restants): " + text
    # print(search_term)de cada usuario en su respectivo lenguaje original. Cada texto estÃ¡ separado por una tabulaciÃ³n, l
    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": gpt_instruction}
        ]
    )
    result = completion.choices[0].message.content
    # print(result)
    return result


# def retrieve_user_brief_description(loaded_df):
#    all_text=[]
#    for tweet, user_description in zip(loaded_df['Tweets'], loaded_df['User description']):
#        concatenated_text = ''
#        concatenated_text += user_description.replace("\n","") + '\n'
#        counter=0
#        print("tweets is", tweet)
#        if (type(tweet) == list):
#          for t in tweet:
#        	  print ("T is ",t)
#        	  if counter<10:
#        		  concatenated_text += t['text'].replace("\n","") + '\n' 
#        		  counter+=1
#        else:
#        	if counter<10:
#        		  concatenated_text += tweet.replace("\n","") + '\n' 
#        		  counter+=1
#            
#        gpt_description=gpt_request_description(concatenated_text)
#        #print(gpt_description)
#        all_text.append(gpt_description) 
#    loaded_df["user_summary"]=all_text
#        
#    return loaded_df

def retrieve_user_brief_description(loaded_df):
    all_text = []
    for tweet, user_description in zip(loaded_df['tweets'], loaded_df['description']):
        concatenated_text = ''
        concatenated_text += user_description.replace("\n", "") + '\n'
        counter = 0
        for t in tweet:
            if counter < 10:
                concatenated_text += t['text'].replace("\n", "") + '\n'

            counter += 1

        gpt_description = gpt_request_description(concatenated_text)
        # print(gpt_description)
        all_text.append(gpt_description)
    loaded_df["user_summary"] = all_text

    return loaded_df


def get_user_brief_description(parent_folder_path):
    # Iterate over each folder within the parent folder
    counter = 0
    for folder_name in os.listdir(parent_folder_path):
        # Construct the full path of the folder
        folder_path = os.path.join(parent_folder_path, folder_name)

        # Check if the item is a folder
        if os.path.isdir(folder_path):
            # Process the folder
            print(f"Processing folder: {folder_name}")

            # Iterate over each file within the folder
            for file_name in os.listdir(folder_path):

                if "users_features_" in file_name:
                    # time.sleep(10)
                    file_path = os.path.join(folder_path, file_name)
                    df_rank = pd.read_json(file_path, orient='records', lines=True, convert_dates=['Date Created'])
                    df_complete = retrieve_user_brief_description(df_rank)
                    print("printing " + file_path.replace('.json', '') + '_summary_added.json')
                    df_complete.to_json(file_path.replace('.json', '') + '_summary_added.json', orient='records',
                                        lines=True)
                    # counter+=1
# parent_folder_path = r'C:\Users\USUARIO\Desktop\REMISS - copia\datos_precalculados'
# get_user_brief_description(parent_folder_path)
